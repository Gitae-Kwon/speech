# app.py
# -*- coding: utf-8 -*-
import os
from datetime import date
from dateutil.relativedelta import relativedelta
from urllib.parse import quote

import streamlit as st
import pandas as pd
import numpy as np
import requests
from requests.adapters import HTTPAdapter, Retry
from pytrends.request import TrendReq

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="AI Tools Tracker (Persistent)", page_icon="ğŸ“Š", layout="wide")

DATA_DIR = "./data"
FIG_DIR = "./figures"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(FIG_DIR, exist_ok=True)

# ì¶”ì  ê¸°ë³¸ê°’
DEFAULT_REGION = "US"     # '', 'US', 'KR', ...
DEFAULT_START = date(2022, 1, 1)
DEFAULT_END = date.today()

# 6ê°œ ë¶„ì•¼ì™€ í‚¤ì›Œë“œ(ì›í•˜ë©´ ììœ ë¡­ê²Œ ìˆ˜ì •/ì¶”ê°€)
CATEGORIES = {
    "1. ìƒì‚°ì„±/ì—…ë¬´ë³´ì¡°": ["ChatGPT", "Microsoft Copilot", "Google Gemini", "Notion AI", "Grammarly"],
    "2. ë§ˆì¼€íŒ…": ["Jasper", "Copy.ai", "Anyword", "HubSpot AI", "Grammarly Business"],
    "3. ë””ìì¸/ì˜ìƒ/ì´ë¯¸ì§€": ["Canva", "Midjourney", "Adobe Firefly", "Runway", "DALLÂ·E"],
    "4. ê°œë°œ/ì½”ë”©": ["GitHub Copilot", "ChatGPT", "Cursor", "Codeium", "Claude"],
    "5. ê³ ê°ì„œë¹„ìŠ¤/ì±—ë´‡": ["Zendesk AI", "Intercom", "Salesforce Einstein", "Dialogflow", "ChatGPT"],
    "6. ìš´ì˜/ìë™í™”": ["Zapier", "Make", "UiPath", "Power Automate", "n8n"]
}
# ìœ„í‚¤ ë¬¸ì„œ ë§¤í•‘(ì˜ë¬¸ ìœ„í‚¤)
WIKI_PAGES_DEFAULT = {
    "ChatGPT": "ChatGPT",
    "Microsoft Copilot": "Microsoft_Copilot",
    "Google Gemini": "Google_Gemini",
    "Notion AI": "Notion_(product)",  # í†µí•© í˜ì´ì§€ ì¡°íšŒìˆ˜ë¡œ ëŒ€ëµì  ì¸ì§€ë„ ì¶”ì 
    "Grammarly": "Grammarly",

    "Jasper": "Jasper_(software)",    # í•„ìš” ì‹œ ì •í™• ë¬¸ì„œëª…ìœ¼ë¡œ ìˆ˜ì •
    "Copy.ai": "Copy.ai",             # ì—†ìœ¼ë©´ ë¹ˆ ì‹œë¦¬ì¦ˆ ì²˜ë¦¬ë¨
    "Anyword": "Anyword",
    "HubSpot AI": "HubSpot",
    "Grammarly Business": "Grammarly",

    "Canva": "Canva",
    "Midjourney": "Midjourney",
    "Adobe Firefly": "Adobe_Firefly",
    "Runway": "Runway_(company)",
    "DALLÂ·E": "DALL-E",

    "GitHub Copilot": "GitHub_Copilot",
    "Cursor": "Cursor_(software)",    # ì—†ìœ¼ë©´ ë¹ˆ ì²˜ë¦¬
    "Codeium": "Codeium",
    "Claude": "Claude_(language_model)",

    "Zendesk AI": "Zendesk",
    "Intercom": "Intercom_(company)",
    "Salesforce Einstein": "Salesforce_Einstein",
    "Dialogflow": "Dialogflow",

    "Zapier": "Zapier",
    "Make": "Integromat",
    "UiPath": "UiPath",
    "Power Automate": "Power_Automate",
    "n8n": "N8n"
}

# Wikimedia ìš”ì²­ ì„¸ì…˜ (User-Agent í•„ìˆ˜)
SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "AI-Tools-Tracker/1.0 (contact: your_email@example.com)"  # ë³¸ì¸ ì´ë©”ì¼/ê¹ƒí—™ ì´ìŠˆ URL ë“±ìœ¼ë¡œ êµì²´
})
retries = Retry(
    total=5, backoff_factor=1.5,
    status_forcelist=[403, 429, 500, 502, 503, 504],
    allowed_methods=["GET"]
)
SESSION.mount("https://", HTTPAdapter(max_retries=retries))

# =========================
# ìœ í‹¸
# =========================
def ensure_month(dt: date) -> pd.Timestamp:
    return pd.Timestamp(year=dt.year, month=dt.month, day=1)

def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i+n]

# =========================
# Google Trends
# =========================
def build_pytrends():
    return TrendReq(hl="en-US", tz=360)

@st.cache_data(ttl=60*60)
def fetch_google_trends_monthly_mean(keywords, start: date, end: date, region: str = "") -> pd.DataFrame:
    """í‚¤ì›Œë“œë¥¼ 5ê°œ ì´í•˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ì§‘ â†’ ì›” í‰ê· ìœ¼ë¡œ ë¦¬ìƒ˜í”Œ"""
    if not keywords:
        return pd.DataFrame()
    time_window = f"{start.isoformat()} {end.isoformat()}"
    pytrends = build_pytrends()
    frames = []
    for batch in chunks(keywords, 5):
        try:
            pytrends.build_payload(batch, timeframe=time_window, geo=region)
            df = pytrends.interest_over_time()
            if df is None or df.empty:
                continue
            if "isPartial" in df.columns:
                df = df.drop(columns=["isPartial"])
            df = df.resample("MS").mean().round(2)
            frames.append(df)
        except Exception as e:
            st.warning(f"âš ï¸ Google Trends ì¼ë¶€ ì‹¤íŒ¨: {batch} | {e}")
            continue
    if not frames:
        return pd.DataFrame()
    base = frames[0]
    for f in frames[1:]:
        base = base.join(f, how="outer")
    base.index.name = "month"
    # ì›ë˜ í‚¤ì›Œë“œ ìˆœì„œëŒ€ë¡œ
    cols = [k for k in keywords if k in base.columns]
    return base[cols].sort_index()

# =========================
# Wikimedia Pageviews
# =========================
def wiki_month_bounds(start: date, end: date):
    start_str = pd.Timestamp(start).strftime("%Y%m01")
    end_last = (pd.Timestamp(end) + relativedelta(day=31)).strftime("%Y%m%d")
    return start_str, end_last

def wiki_url(project, access, agent, article, start_str, end_last):
    encoded_title = quote(article, safe="")
    return (f"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/"
            f"{project}/{access}/{agent}/{encoded_title}/monthly/{start_str}/{end_last}")

def fetch_wiki_one(title: str, start: date, end: date,
                   project="en.wikipedia", access="all-access", agent="user") -> pd.Series:
    start_str, end_last = wiki_month_bounds(start, end)
    url = wiki_url(project, access, agent, title, start_str, end_last)
    try:
        r = SESSION.get(url, timeout=30)
        r.raise_for_status()
        items = r.json().get("items", [])
        if not items:
            return pd.Series(dtype="float64")
        data = {}
        for it in items:
            ts = str(it["timestamp"])
            y, m = int(ts[:4]), int(ts[4:6])
            data[pd.Timestamp(year=y, month=m, day=1)] = it["views"]
        return pd.Series(data).sort_index()
    except Exception as e:
        st.info(f"â„¹ï¸ ìœ„í‚¤ ì¡°íšŒ ì‹¤íŒ¨: {title} | {e}")
        return pd.Series(dtype="float64")

@st.cache_data(ttl=60*60)
def fetch_wiki_map(page_map: dict, start: date, end: date) -> pd.DataFrame:
    if not page_map:
        return pd.DataFrame()
    series = []
    for key, title in page_map.items():
        s = fetch_wiki_one(title, start, end)
        s.name = key
        series.append(s)
    if not series:
        return pd.DataFrame()
    df = pd.concat(series, axis=1)
    df.index.name = "month"
    return df.sort_index()

# =========================
# ì§€ì† ì¶”ì (íˆìŠ¤í† ë¦¬ ëˆ„ì ) ë¡œì§
# =========================
def load_history(path: str) -> pd.DataFrame:
    if os.path.exists(path):
        try:
            df = pd.read_csv(path, parse_dates=["month"])
            df["month"] = pd.to_datetime(df["month"]).dt.to_period("M").dt.to_timestamp()
            df = df.set_index("month").sort_index()
            return df
        except Exception:
            return pd.DataFrame()
    return pd.DataFrame()

def save_history(path: str, df: pd.DataFrame):
    if df.empty: 
        return
    out = df.copy()
    out = out.sort_index()
    out.index.name = "month"
    out.to_csv(path, index=True)

def merge_history(history: pd.DataFrame, new_df: pd.DataFrame) -> pd.DataFrame:
    if history.empty:
        return new_df
    merged = history.combine_first(new_df).copy()
    # ìƒˆ ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸°(ë™ì›” ê°±ì‹ )
    for col in new_df.columns:
        merged[col].update(new_df[col])
    return merged

# =========================
# ì ìˆ˜ ì‚°ì¶œ(ì •ê·œí™” + ê°€ì¤‘í•©)
# =========================
def minmax_norm(s: pd.Series) -> pd.Series:
    if s.dropna().empty:
        return s
    mn, mx = s.min(), s.max()
    if mx == mn:
        return s * 0  # ì „ë¶€ ë™ì¼í•˜ë©´ 0ìœ¼ë¡œ
    return (s - mn) / (mx - mn)

def zscore_norm(s: pd.Series) -> pd.Series:
    if s.dropna().empty:
        return s
    mu, sd = s.mean(), s.std(ddof=0)
    if sd == 0:
        return s * 0
    return (s - mu) / sd

def compute_scores(trends_df: pd.DataFrame, wiki_df: pd.DataFrame,
                   w_trends: float = 0.6, w_wiki: float = 0.4) -> pd.DataFrame:
    """
    ì›”ë³„/ë„êµ¬ë³„ ì ìˆ˜ ì‚°ì¶œ:
    - Trends: min-max ì •ê·œí™”
    - Wiki: z-score â†’ min-maxë¡œ í•œ ë²ˆ ë” ìŠ¤ì¼€ì¼ë§(ìŒìˆ˜/ìŠ¤ì¼€ì¼ ì°¨ì´ ë³´ì •)
    - ë³µí•© ì ìˆ˜ = 0.6*Trends_norm + 0.4*Wiki_norm
    """
    all_tools = sorted(list(set(trends_df.columns.tolist()) | set(wiki_df.columns.tolist())))
    idx = trends_df.index.union(wiki_df.index).sort_values()
    trends_norm = pd.DataFrame(index=idx, columns=all_tools, dtype=float)
    wiki_norm = pd.DataFrame(index=idx, columns=all_tools, dtype=float)

    for t in all_tools:
        if t in trends_df.columns:
            trends_norm[t] = minmax_norm(trends_df[t])
        if t in wiki_df.columns:
            wiki_norm[t] = minmax_norm(zscore_norm(wiki_df[t]))

    score = (w_trends * trends_norm.fillna(0) + w_wiki * wiki_norm.fillna(0))
    score.index.name = "month"
    return score

# =========================
# UI - ì‚¬ì´ë“œë°”
# =========================
st.title("ğŸ“Š AI ë„êµ¬ ì§€ì† ì¶”ì  & ë¶„ì•¼ë³„ ë¦¬ë”ë³´ë“œ")
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    region = st.selectbox("Google Trends ì§€ì—­ (ë¹ˆê°’=ê¸€ë¡œë²Œ)", ["", "US", "KR", "BR", "JP", "GB", "DE", "FR"], index=1)
    start = st.date_input("ì‹œì‘ì¼", value=DEFAULT_START, min_value=date(2019,1,1), max_value=DEFAULT_END)
    end = st.date_input("ì¢…ë£Œì¼", value=DEFAULT_END, min_value=DEFAULT_START, max_value=DEFAULT_END)
    use_wiki = st.checkbox("Wikipedia Pageviews ì‚¬ìš©", value=True)
    st.markdown("---")
    st.caption("Tip: ì²˜ìŒì—” ê¸°ë³¸ê°’ ê·¸ëŒ€ë¡œ ì‹¤í–‰ â†’ ì´í›„ ì‚¬ì´ë“œë°”ì—ì„œ ì¡°ì •")

# ì¶”ì  ëŒ€ìƒ í‚¤ì›Œë“œ/ìœ„í‚¤ ë§¤í•‘ ìƒì„±
ALL_TOOLS = sorted({tool for v in CATEGORIES.values() for tool in v})
WIKI_MAP = {k: WIKI_PAGES_DEFAULT.get(k, k.replace(" ", "_")) for k in ALL_TOOLS}

# =========================
# ë°ì´í„° ìˆ˜ì§‘ (ì´ë²ˆ ì‹¤í–‰ë¶„)
# =========================
with st.spinner("Google Trends ìˆ˜ì§‘ ì¤‘..."):
    trends_now = fetch_google_trends_monthly_mean(ALL_TOOLS, start, end, region)

if use_wiki:
    with st.spinner("Wikipedia Pageviews ìˆ˜ì§‘ ì¤‘..."):
        wiki_now = fetch_wiki_map(WIKI_MAP, start, end)
else:
    wiki_now = pd.DataFrame()

# =========================
# íˆìŠ¤í† ë¦¬ ë¡œë“œ & ë³‘í•© & ì €ì¥
# =========================
hist_trends_path = os.path.join(DATA_DIR, f"history_trends_{region or 'GLOBAL'}.csv")
hist_wiki_path = os.path.join(DATA_DIR, "history_wiki.csv")
hist_score_path = os.path.join(DATA_DIR, f"history_scores_{region or 'GLOBAL'}.csv")

hist_trends = load_history(hist_trends_path)
hist_wiki = load_history(hist_wiki_path)

trends_hist_new = merge_history(hist_trends, trends_now) if not trends_now.empty else hist_trends
wiki_hist_new = merge_history(hist_wiki, wiki_now) if not wiki_now.empty else hist_wiki

# ì €ì¥
save_history(hist_trends_path, trends_hist_new)
if use_wiki:
    save_history(hist_wiki_path, wiki_hist_new)

# =========================
# ì ìˆ˜ ê³„ì‚° & ì €ì¥
# =========================
score_hist = compute_scores(trends_hist_new if not trends_hist_new.empty else pd.DataFrame(),
                            wiki_hist_new if not wiki_hist_new.empty else pd.DataFrame(),
                            w_trends=0.6, w_wiki=0.4)
save_history(hist_score_path, score_hist)

# =========================
# í™”ë©´ í‘œì‹œ
# =========================
c1, c2 = st.columns(2)
with c1:
    st.subheader("Google Trends (ì›” í‰ê· , ìƒëŒ€ì§€í‘œ 0â€“100)")
    if not trends_hist_new.empty:
        st.line_chart(trends_hist_new[ALL_TOOLS].tail(36), height=320, use_container_width=True)
        st.dataframe(trends_hist_new.tail(12), use_container_width=True)
        st.download_button("â¬‡ï¸ Trends íˆìŠ¤í† ë¦¬ CSV", trends_hist_new.to_csv().encode("utf-8"),
                           file_name=os.path.basename(hist_trends_path), mime="text/csv")
    else:
        st.info("Trends ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with c2:
    st.subheader("Wikipedia Pageviews (ì›”ë³„ ì ˆëŒ€ ì¡°íšŒìˆ˜)")
    if use_wiki and not wiki_hist_new.empty:
        st.line_chart(wiki_hist_new[ALL_TOOLS].tail(36), height=320, use_container_width=True)
        st.dataframe(wiki_hist_new.tail(12), use_container_width=True)
        st.download_button("â¬‡ï¸ Wiki íˆìŠ¤í† ë¦¬ CSV", wiki_hist_new.to_csv().encode("utf-8"),
                           file_name=os.path.basename(hist_wiki_path), mime="text/csv")
    else:
        st.info("Wiki ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

st.markdown("---")
st.subheader("ğŸ§® ë³µí•© ì ìˆ˜(ì •ê·œí™” ê²°í•©) íˆìŠ¤í† ë¦¬")
if not score_hist.empty:
    st.line_chart(score_hist[ALL_TOOLS].tail(36), height=320, use_container_width=True)
    st.dataframe(score_hist.tail(12), use_container_width=True)
    st.download_button("â¬‡ï¸ Score íˆìŠ¤í† ë¦¬ CSV", score_hist.to_csv().encode("utf-8"),
                       file_name=os.path.basename(hist_score_path), mime="text/csv")
else:
    st.info("ì ìˆ˜ íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

# =========================
# ë¶„ì•¼ë³„ ë¦¬ë”ë³´ë“œ (ìµœê·¼ ì›” Top N)
# =========================
st.markdown("---")
st.header("ğŸ† ë¶„ì•¼ë³„ ë¦¬ë”ë³´ë“œ (ìµœê·¼ ì›” ê¸°ì¤€)")
top_n = st.slider("Top N", min_value=3, max_value=10, value=5, step=1)

if not score_hist.empty:
    latest_month = score_hist.index.max()
    st.caption(f"ìµœê·¼ ì›”: **{latest_month.strftime('%Y-%m')}**")
    lb_cols = st.columns(3)
    i = 0
    for cat, tools in CATEGORIES.items():
        sub = score_hist.loc[latest_month, score_hist.columns.intersection(tools)].sort_values(ascending=False)
        df_show = pd.DataFrame({"Rank": range(1, len(sub)+1), "Tool": sub.index, "Score": sub.values}).head(top_n)
        with lb_cols[i % 3]:
            st.markdown(f"**{cat}**")
            st.dataframe(df_show, use_container_width=True, hide_index=True)
        i += 1
else:
    st.info("ì ìˆ˜ ë°ì´í„°ê°€ ì—†ì–´ ë¦¬ë”ë³´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# =========================
# ì°¸ê³ /ì£¼ì˜
# =========================
with st.expander("â„¹ï¸ ì°¸ê³  ë° ì£¼ì˜ì‚¬í•­"):
    st.markdown("""
- **ì§€ì† ì¶”ì  ë°©ì‹**: ì‹¤í–‰ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ìƒˆ ì›” ë°ì´í„°ê°€ ìˆìœ¼ë©´ history CSVì— ë³‘í•©/ê°±ì‹ í•©ë‹ˆë‹¤. (ë‹¤ë¥¸ ë‹¬ì€ ê¸°ì¡´ ê°’ ìœ ì§€)
- **ë³µí•© ì ìˆ˜**: Trends(min-max) 0.6 + Wiki(zscoreâ†’min-max) 0.4ì˜ ê°€ì¤‘í•©ì…ë‹ˆë‹¤. í•„ìš” ì‹œ ì‚¬ì´ë“œë°” ì˜µì…˜ìœ¼ë¡œ ë°”ê¾¸ë„ë¡ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- **Google TrendsëŠ” ìƒëŒ€ì§€í‘œ**ì´ë¯€ë¡œ, ìš”ì²­ ì„¸íŠ¸ê°€ ë°”ë€Œë©´ ìŠ¤ì¼€ì¼ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì¼ í‚¤ì›Œë“œ ì„¸íŠ¸ë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
- **Wikipedia Pageviews**ëŠ” ì ˆëŒ€ ì¡°íšŒìˆ˜ë¼ ê·œëª¨ê°ì„ ë³´ì™„í•˜ì§€ë§Œ, ì¼ë¶€ ë„êµ¬ëŠ” ì •í™•í•œ ìœ„í‚¤ ë¬¸ì„œê°€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê·¸ ê²½ìš° ë¹ˆê°’ ì²˜ë¦¬).
- **403/429** ë°©ì§€ë¥¼ ìœ„í•´ User-Agentë¥¼ **ë³¸ì¸ ì—°ë½ì²˜**ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ ê¼­ ë°”ê¾¸ì„¸ìš”.
""")
